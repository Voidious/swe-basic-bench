# SWE-Basic-Bench

A basic benchmark for evaluating the effectiveness of Software Engineering (SWE) coding agents and associated tools (e.g., MCP tools).

## Mission

The goal of this benchmark is to provide a standardized set of coding tasks that can be used to evaluate the performance of AI coding agents. The benchmark is designed to be run within an environment like Cursor, allowing for the evaluation of different AI models and tool configurations.

Performance can be measured across several axes:
- **Task Completion:** Did the agent successfully complete the task?
- **Correctness:** Does the generated code pass all verification checks?
- **Efficiency:** How many steps, tool calls, or interactions were required?
- **Execution Time:** How long did it take for the agent to complete the task (lower is better)?
- **Code Quality:** Is the generated code clean, maintainable, and well-documented?

## Structure

The benchmark is organized into a series of tasks. Each task is self-contained in its own directory within the `tasks/` directory.

```
swe-basic-bench/
├── tasks/
│   ├── <task-name>/
│   │   ├── prompt.md
│   │   ├── initial_code/      (optional)
│   │   ├── solution/          (to be generated by the agent)
│   │   └── verifier/
│   │       └── ... (verification scripts, e.g., pytest tests)
│   └── ...
├── runner.py                (optional, for future automation)
└── README.md
```

### Task Components

- `prompt.md`: A markdown file containing the task description that will be given to the AI agent.
- `initial_code/`: An optional directory containing the starting state of the codebase for the task. If a task starts from scratch, this directory can be omitted.
- `solution/`: This directory is where the agent should write the solution code. It is initially empty.
- `verifier/`: A directory containing scripts to verify the correctness of the generated solution. This typically includes a set of tests.

## How to Run a Task

1.  **Select a Task:** Choose a task from the `tasks/` directory.
2.  **Prepare the Environment:**
    -   Copy the contents of the task's `initial_code/` directory (if it exists) into the `solution/` directory.
    -   The `solution/` directory is the working directory for the agent.
3.  **Instruct the Agent:**
    -   Open the `solution/` directory in your AI-powered editor (e.g., Cursor).
    -   Provide the agent with the content of the corresponding `prompt.md`.
4.  **Verify the Solution:**
    -   Once the agent has completed the task, run the verification script(s) from the `verifier/` directory against the code in `solution/`.
    -   The specifics of running the verifier will be detailed in each task's `prompt.md`.

## How to Add a New Task

1.  Create a new directory under `tasks/` with a descriptive name for your task.
2.  Inside the new task directory, create:
    -   A `prompt.md` file with a clear problem statement.
    -   A `solution/` directory.
    -   A `verifier/` directory with one or more verification scripts.
    -   (Optional) An `initial_code/` directory if the task builds on existing code.
3.  Ensure the `prompt.md` includes instructions on how to run the verifier.
