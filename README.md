# SWE-Basic-Bench

A basic benchmark for evaluating the effectiveness of Software Engineering (SWE) coding agents and associated tools (e.g., MCP tools).

## Mission

The goal of this benchmark is to provide a standardized set of coding tasks that can be used to evaluate the performance of AI coding agents. The benchmark is designed to be run within an environment like Cursor, allowing for the evaluation of different AI models and tool configurations.

Performance can be measured across several axes:
- **Task Completion:** Did the agent successfully complete the task?
- **Correctness:** Does the generated code pass all verification checks?
- **Execution Time:** How long did it take for the agent to complete the task?

## How to Run the Full Benchmark

To run the entire suite of benchmark tasks, the agent should follow this sequence of steps for each task. The agent's goal is to complete all available tasks.

1.  **Confirm README Review**: Before you can run any other command, you must confirm that you have read and understood the instructions in this README. This is a one-time setup step.
    ```bash
    python runner.py --confirm-i-have-viewed-entire-readme
    ```

2.  **List All Tasks**: Get the full list of tasks to be completed.
    ```bash
    python runner.py list
    ```

3.  **Complete Each Task**: For each task identified in the list, the agent must perform the following sub-steps in order:
    
    a. **Start the Task**: This prepares the environment and displays the prompt.
    ```bash
    python runner.py start <task-name>
    ```
    b. **Generate the Solution**: Based on the prompt, the agent must write the code and save it to the `tasks/<task-name>/solution/` directory.

    c. **Evaluate the Solution**: This will run the verifier and record the results.
    ```bash
    python runner.py evaluate <task-name>
    ```

4.  **Generate Final Report**: After attempting all tasks, the agent should generate a consolidated report of all results.
    ```bash
    python runner.py report
    ```

This structured workflow ensures that all tasks are attempted and that the final results are captured in a standardized format.

## Structure

The benchmark is organized into a series of tasks. Each task is self-contained in its own directory within the `tasks/` directory.

```
swe-basic-bench/
├── tasks/
│   ├── <task-name>/
│   │   ├── prompt.md
│   │   ├── initial_code/      (optional)
│   │   ├── solution/          (to be generated by the agent)
│   │   └── verifier/
│   │       └── ... (verification scripts, e.g., pytest tests)
│   └── ...
├── runner.py                (do not modify)
└── README.md
```

### Task Components

- `prompt.md`: A markdown file containing the task description that will be given to the AI agent.
- `initial_code/`: An optional directory containing the starting state of the codebase for the task. If a task starts from scratch, this directory can be omitted.
- `solution/`: This directory is where the agent should write the solution code. It is initially empty.
- `verifier/`: A directory containing scripts to verify the correctness of the generated solution. This typically includes a set of tests.

## How to Run a Task

The official method for running benchmark tasks is to use the `runner.py` script. This ensures that the process is standardized and that results are captured in a uniform way.

**Important:** Do not modify the `runner.py` script. The benchmark is designed to be run with the provided runner to ensure standardized evaluation.

The workflow is designed for a coding agent and is broken into four distinct steps:

1.  **List available tasks**: See what tasks are available to run.
2.  **Start a task**: Prepare the workspace and get the prompt.
3.  **Evaluate the solution**: Run the verifier and get the score.
4.  **Report all results**: See a summary of all results.

### 1. List Available Tasks
To see all the tasks in the benchmark, run:
```bash
python runner.py list
```

### 2. Start a Task
When the agent is ready to begin a task, it should run the `start` command. This will prepare the `solution/` directory and display the prompt.

```bash
python runner.py start <task-name>
```
For example:
```bash
python runner.py start simple-calculator
```

After running this command, the agent should:
1.  Read the displayed prompt.
2.  Generate the solution code and save it to the `tasks/<task-name>/solution/` directory.

### 3. Evaluate the Solution
Once the agent has created the solution, it should run the `evaluate` command. This will:
1.  Run the verifier tests (`pytest`).
2.  Calculate the final scores.
3.  Generate the `results.json` file.

```bash
python runner.py evaluate <task-name>
```
For example:
```bash
python runner.py evaluate simple-calculator
```

### 4. Report All Results
After running one or more evaluations, the agent can see a summary of all results by running the `report` command.

```bash
python runner.py report
```

This will display the scores for all completed tasks and a final total score in a markdown table.

This structured approach allows an agent to systematically work through the benchmark, from understanding the task to seeing the final result.

### How to Report Results

After running the benchmark, the coding agent should report the results to the user. The report should be clear, concise, and provide a summary of the agent's performance.

The `report` command, as described above, will generate this table automatically. The agent should present the final results in a markdown table format. The table should include the following metrics for each task:
- Task Name
- Correctness Score
- Task Completion Score
- Final Score
- Execution Time (in seconds)

The agent can gather this information from the `results.json` file generated for each task.

Here is an example of the expected output format:

| Task                   | Correct. (/60) | Complete (/20) | Score (/80) | Time (s) |
| ---------------------- | -------------- | -------------- | ----------- | -------- |
| `simple-calculator`    | 60.00          | 20.00          | 80.00       | 13.66    |
| `csv-report-generator` | 60.00          | 20.00          | 80.00       | 5.21     |
| **Total**              | **120.00**     | **40.00**      | **160.00**  | **18.87**|

The final row should contain the sum of the scores and execution times for all tasks.

## Scoring

Each task attempt is scored out of 80 points, based on the following objective criteria.

-   **Correctness (60 points):** Points are awarded based on the percentage of passing tests in the `verifier/` script.
    -   `Score = 60 * (Number of Passing Tests / Total Number of Tests)`
-   **Task Completion (20 points):**
    -   **20 points:** The agent produced all the required files and outputs in the correct locations.
    -   **0 points:** The agent failed to produce the required output.

**Execution Time** is automatically calculated by the `runner.py` script and recorded in the `results.json` file. It is not scored directly but serves as a key performance indicator.

### Results Format

The `runner.py` script generates a `results.json` file with the following structure.

```json
{
    "task_name": "task-name-goes-here",
    "timestamp": "2023-10-27T10:00:00Z",
    "scores": {
        "correctness": 60,
        "task_completion": 20
    },
    "final_score_objective": 80,
    "pytest_output": "..."
}
```

## How to Add a New Task

1.  Create a new directory under `tasks/` with a descriptive name for your task.
2.  Inside the new task directory, create:
    -   A `prompt.md` file with a clear problem statement.
    -   A `solution/` directory.
    -   A `verifier/` directory with one or more verification scripts.
    -   (Optional) An `initial_code/` directory if the task builds on existing code.
3.  Ensure the `prompt.md` includes instructions on how to run the verifier.
