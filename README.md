## For Human Users

1. Clone this repository.
2. Open the repository in an AI coding assistant (e.g., Cursor).
3. Instruct the agent to execute the benchmark.

# SWE-Basic-Bench

A basic benchmark for evaluating the effectiveness of Software Engineering (SWE) coding agents and associated tools (e.g., MCP tools).

## Mission

The goal of this benchmark is to provide a standardized set of coding tasks that can be used to evaluate the performance of AI coding agents. The benchmark is designed to be run within an environment like Cursor, allowing for the evaluation of different AI models and tool configurations.

Performance can be measured across several axes:
- **Task Completion:** Did the agent successfully complete the task?
- **Correctness:** Does the generated code pass all verification checks?
- **Execution Time:** How long did it take for the agent to complete the task?

## How to Run the Benchmark

**These instructions are for an AI coding agent.**

To run the suite of benchmark tasks, you **must** use the `runner.py` script. This is the only way to ensure that the process is standardized and that results are captured correctly.

**CRITICAL: You must follow these steps in the exact order listed. Do not skip any steps. Failure to follow these instructions will result in a failed task.**

**Important:** Do not modify the `runner.py` script or any of the `verifier/` scripts. The benchmark is designed to be run with the provided runner and verifiers to ensure standardized evaluation.

The required workflow to complete all available tasks is as follows:

1.  **Confirm README Review (MANDATORY)**
    Before you can run any other command, you **must** confirm that you have read and understood the instructions in this README. This is a mandatory, one-time setup step.
    ```bash
    python runner.py --confirm-i-have-viewed-entire-readme
    ```
    You will not be able to proceed until this command is run successfully.

2.  **List All Tasks**
    Get the full list of tasks to be completed.
    ```bash
    python runner.py list
    ```

3.  **Complete Each Task**
    For each task identified in the list, you **must** perform the following sub-steps **in order**.

    a. **Start the Task**: This step is **required** to see the task prompt. It prepares the environment and displays the full requirements. **DO NOT attempt to solve a task without running `start` first.** You will not have the correct information.
    ```bash
    python runner.py start <task-name>
    ```
    After starting the task, read the prompt that is displayed, generate the solution code, and save it to the `tasks/<task-name>/solution/` directory.

    b. **Evaluate the Solution**: After you have saved your solution, run this command to execute the verifier, calculate scores, and record the results in `results.json`.
    ```bash
    python runner.py evaluate <task-name>
    ```

4.  **Generate Final Report**
    After attempting all tasks, generate a consolidated report of all results.
    ```bash
    python runner.py report
    ```

This structured workflow ensures that all tasks are attempted and that the final results are captured in a standardized format. Deviating from this workflow will lead to incorrect results.

## Structure

The benchmark is organized into a series of tasks. Each task is self-contained in its own directory within the `tasks/` directory.

```
swe-basic-bench/
├── tasks/
│   ├── <task-name>/
│   │   ├── prompt.md
│   │   ├── initial_code/      (optional)
│   │   ├── solution/          (to be generated by the agent)
│   │   └── verifier/
│   │       └── ... (verification scripts, e.g., pytest tests) (do not modify)
│   └── ...
├── runner.py                (do not modify)
└── README.md
```

### Task Components

- `prompt.md`: A markdown file containing the task description that will be given to the AI agent.
- `initial_code/`: An optional directory containing the starting state of the codebase for the task. If a task starts from scratch, this directory can be omitted.
- `solution/`: This directory is where the agent should write the solution code. It is initially empty.
- `verifier/`: A directory containing scripts to verify the correctness of the generated solution. This typically includes a set of tests. These scripts should not be modified.

## How to Report Results

After running the benchmark, the coding agent should report the results to the user. The report should be clear, concise, and provide a summary of the agent's performance.

The `report` command, as described above, will generate this table automatically. The agent should present the final results in a markdown table format. The table should include the following metrics for each task:
- Task Name
- Correctness Score
- Task Completion Score
- Final Score
- Execution Time (in seconds)

The agent can gather this information from the `results.json` file generated for each task.

Here is an example of the expected output format:

| Task                           | Correct. (/60) | Complete (/20) | Score (/80) | Time (s) |
| ------------------------------ | -------------- | -------------- | ----------- | -------- |
| `simple-calculator`            | 60.00          | 20.00          | 80.00       | 13.66    |
| `csv-report-generator`         | 60.00          | 20.00          | 80.00       | 5.21     |
| `data-pipeline-with-branching` | 60.00          | 20.00          | 80.00       | 8.42     |
| **Total**                      | **180.00**     | **60.00**      | **240.00**  | **27.29**|

The final row should contain the sum of the scores and execution times for all tasks.

## Scoring

Each task attempt is scored out of 80 points, based on the following objective criteria.

-   **Correctness (60 points):** Points are awarded based on the percentage of passing tests in the `verifier/` script.
    -   `Score = 60 * (Number of Passing Tests / Total Number of Tests)`
-   **Task Completion (20 points):**
    -   **20 points:** The agent produced all the required files and outputs in the correct locations.
    -   **0 points:** The agent failed to produce the required output.

**Execution Time** is automatically calculated by the `runner.py` script and recorded in the `results.json` file. It is not scored directly but serves as a key performance indicator.

### Results Format

The `runner.py` script generates a `results.json` file with the following structure.

```json
{
    "task_name": "task-name-goes-here",
    "timestamp": "2023-10-27T10:00:00Z",
    "scores": {
        "correctness": 60,
        "task_completion": 20
    },
    "final_score_objective": 80,
    "pytest_output": "..."
}
```

## How to Add a New Task

1.  Create a new directory under `tasks/` with a descriptive name for your task.
2.  Inside the new task directory, create:
    -   A `prompt.md` file with a clear problem statement.
    -   A `solution/` directory.
    -   A `verifier/` directory with one or more verification scripts.
    -   (Optional) An `initial_code/` directory if the task builds on existing code.
3.  Ensure the `prompt.md` includes instructions on how to run the verifier.

## Benchmark Tasks

This benchmark currently includes the following tasks:

- **simple-calculator**: Implement a basic calculator that can perform addition, subtraction, multiplication, and division based on user input from the command line.
- **command-line-todo-list**: Build a command-line to-do list application that supports adding, listing, and removing tasks, with persistent storage.
- **csv-report-generator**: Generate summary and detailed reports from a CSV file, including calculations and formatted output.
- **data-pipeline-with-branching**: Build an automated data pipeline that downloads, validates, processes, and reports on CSV data. The pipeline must handle conditional branching (e.g., process only 'active' rows if a 'status' column exists), error recovery (e.g., retry downloads, halt on validation errors), and be able to report its current state and progress. This task is designed to test multi-step reasoning, dependency tracking, error recovery, and reflection capabilities, making it especially suitable for agents with advanced planning or sequential-thinking tools.
